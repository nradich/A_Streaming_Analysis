{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrinz5hLlHSBNyeDVSbNy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nradich/A_Streaming_Analysis/blob/collab_test/StreamingPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1)**\n",
        "\n",
        " - Query the JustWatchAPI to get library of available streaming content.\n",
        " - Reached out to the API team, will see what they say\n",
        "\n",
        " - Would then extract data from the API, and store it probably as a CSV in the drive"
      ],
      "metadata": {
        "id": "fKYdECaHgojn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tm8S95BYgf59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 2)**\n",
        "\n",
        "- Read in the Personsas Dataset from Nvidia"
      ],
      "metadata": {
        "id": "zfgxKXBWhF2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5)\n",
        "Make sure can hookup to the transformers library and get an LLM\n",
        "\n",
        "Transformers is the hugging face API, would specifgy the model there"
      ],
      "metadata": {
        "id": "HqyrAZDribP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers torch # torch is the backend, sentence-transformers for embeddings"
      ],
      "metadata": {
        "id": "_aK-i2nLgcgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 1. Choose a model ID from Hugging Face Hub\n",
        "# For a smaller, fast example: \"gpt2\"\n",
        "# For something more capable (but larger): \"distilgpt2\", \"microsoft/DialoGPT-small\",\n",
        "# or for more recent, look into instruction-tuned models like \"google/gemma-2b-it\" (requires agreement)\n",
        "# or \"meta-llama/Llama-2-7b-chat-hf\" (requires agreement)\n",
        "\n",
        "# Let's start with GPT-2 for a quick demonstration\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = 0 if torch.cuda.is_available() else -1 # 0 for first GPU, -1 for CPU\n",
        "\n",
        "# Option 1: Using the `pipeline` API (simplest for common tasks)\n",
        "# This handles tokenizer and model loading automatically for many tasks\n",
        "print(f\"Loading model '{model_name}' using pipeline...\")\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    torch_dtype=torch.float16, # Use float16 for memory efficiency on GPU\n",
        "    device=device\n",
        ")\n",
        "print(\"Model loaded via pipeline!\")\n",
        "\n",
        "# Example usage with pipeline:\n",
        "prompt = \"Given a persona who loves action movies and sci-fi, which streaming service would they choose?\"\n",
        "output = generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "print(\"\\nLLM Prediction (Pipeline):\")\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "10hK22_ikowL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQgc5JTgn164"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}